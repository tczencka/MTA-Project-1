{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# various options in pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import HTTPError\n",
    "\n",
    "def load_turnstile_data(urls):\n",
    "    dfs = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            df = pd.read_csv(url)\n",
    "            dfs.append(df)\n",
    "        except HTTPError as err:\n",
    "            if err.code == 404:\n",
    "                continue\n",
    "            else:\n",
    "                raise err\n",
    "    return pd.concat(dfs).reset_index()\n",
    "\n",
    "def format_day_month(num):\n",
    "    return '0' + str(num) if num < 10 else str(num)\n",
    "\n",
    "def format_year(num):\n",
    "    return str(num)[-2:]\n",
    "\n",
    "def generate_url(date):\n",
    "    y = format_year(date.year)\n",
    "    m = format_day_month(date.month)\n",
    "    d = format_day_month(date.day)\n",
    "    return f'http://web.mta.info/developers/data/nyct/turnstile/turnstile_{y}{m}{d}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_for_week_range(start, end):\n",
    "    assert pd.to_datetime(start).weekday() == 5, \"Start must be a Saturday to match turnstile data\"\n",
    "    assert pd.to_datetime(start).weekday() == 5, \"End must be a Saturday to match turnstile data\"\n",
    "    \n",
    "    urls = list(map(generate_url, pd.date_range(start, end, freq=\"7D\")))\n",
    "    return load_turnstile_data(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_data = fetch_data_for_week_range('3/2/2019', '3/30/2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mta_data_march.pickle', 'wb') as write_file:\n",
    "    pickle.dump(mta_data, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1016285, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mta_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_num_rows = mta_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mta_data_march.pickle', 'rb') as read_file:\n",
    "    mta_data = pickle.load(read_file).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'index', 'C/A', 'UNIT', 'SCP', 'STATION', 'LINENAME',\n",
       "       'DIVISION', 'DATE', 'TIME', 'DESC', 'ENTRIES',\n",
       "       'EXITS                                                               '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mta_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_data.columns = [col.strip() for col in mta_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_data = mta_data.sort_values(['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Check For Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.1 How many duplicates are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_unique_count = mta_data.groupby(['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME']).agg({ 'ENTRIES': 'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 57 duplicate entries\n",
    "by_unique_count[by_unique_count.ENTRIES > 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.2 Duplicates Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ: I considered putting a summary of what was discovered at the top rather than the bottom but wasn't sure.\n",
    "# What's best practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_data['num_dups'] = mta_data.groupby(['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME'])['ENTRIES'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_dups = mta_data[mta_data.num_dups > 1].sort_values(['C/A', 'UNIT', 'SCP', 'STATION', 'num_dups'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 14)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_dups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>ENTRIES</th>\n",
       "      <th>EXITS</th>\n",
       "      <th>num_dups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>114.000</td>\n",
       "      <td>114.000</td>\n",
       "      <td>114.000</td>\n",
       "      <td>114.000</td>\n",
       "      <td>114.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>452651.798</td>\n",
       "      <td>75263.798</td>\n",
       "      <td>7068988.395</td>\n",
       "      <td>7762899.395</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>409794.858</td>\n",
       "      <td>38036.586</td>\n",
       "      <td>6172817.503</td>\n",
       "      <td>6717998.744</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>53585.000</td>\n",
       "      <td>5067.000</td>\n",
       "      <td>2471.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>53752.250</td>\n",
       "      <td>53694.250</td>\n",
       "      <td>2114976.000</td>\n",
       "      <td>1451309.750</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>146109.500</td>\n",
       "      <td>53902.500</td>\n",
       "      <td>5870442.000</td>\n",
       "      <td>7068961.500</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>911929.750</td>\n",
       "      <td>100251.750</td>\n",
       "      <td>9699393.750</td>\n",
       "      <td>10685032.750</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>911959.000</td>\n",
       "      <td>146153.000</td>\n",
       "      <td>18100105.000</td>\n",
       "      <td>18839460.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         level_0      index      ENTRIES        EXITS  num_dups\n",
       "count    114.000    114.000      114.000      114.000   114.000\n",
       "mean  452651.798  75263.798  7068988.395  7762899.395     2.000\n",
       "std   409794.858  38036.586  6172817.503  6717998.744     0.000\n",
       "min    53585.000   5067.000     2471.000        0.000     2.000\n",
       "25%    53752.250  53694.250  2114976.000  1451309.750     2.000\n",
       "50%   146109.500  53902.500  5870442.000  7068961.500     2.000\n",
       "75%   911929.750 100251.750  9699393.750 10685032.750     2.000\n",
       "max   911959.000 146153.000 18100105.000 18839460.000     2.000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_dups.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['REGULAR', 'RECOVR AUD'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_dups.DESC.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>C/A</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>SCP</th>\n",
       "      <th>STATION</th>\n",
       "      <th>LINENAME</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>ENTRIES</th>\n",
       "      <th>EXITS</th>\n",
       "      <th>num_dups</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DESC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RECOVR AUD</th>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGULAR</th>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            level_0  index  C/A  UNIT  SCP  STATION  LINENAME  DIVISION  DATE  \\\n",
       "DESC                                                                            \n",
       "RECOVR AUD       57     57   57    57   57       57        57        57    57   \n",
       "REGULAR          57     57   57    57   57       57        57        57    57   \n",
       "\n",
       "            TIME  ENTRIES  EXITS  num_dups  \n",
       "DESC                                        \n",
       "RECOVR AUD    57       57     57        57  \n",
       "REGULAR       57       57     57        57  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_dups.groupby(['DESC']).agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "across_desc = pd.pivot_table(with_dups,\n",
    "               values='ENTRIES',\n",
    "               columns='DESC',\n",
    "               index=['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME'],\n",
    "               aggfunc='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "across_desc['dup_diff'] = across_desc['RECOVR AUD'] - across_desc['REGULAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count          57.000\n",
       "mean     -4023566.404\n",
       "std       6243779.993\n",
       "min     -10314855.000\n",
       "25%     -10314674.000\n",
       "50%      -5383636.000\n",
       "75%        131439.000\n",
       "max      16148594.000\n",
       "Name: dup_diff, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "across_desc['dup_diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_data['prev_entries'] = mta_data.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['ENTRIES'].transform(lambda grp: grp.shift(1))\n",
    "mta_data['prev_desc'] = mta_data.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['DESC'].transform(lambda grp: grp.shift(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mta_data['next_entries'] = mta_data.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['ENTRIES'].transform(lambda grp: grp.shift(-1))\n",
    "mta_data['next_desc'] = mta_data.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['DESC'].transform(lambda grp: grp.shift(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "across_desc = across_desc.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "across_desc = across_desc.merge(\n",
    "    mta_data[['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME', 'DESC', 'prev_entries', 'prev_desc', 'next_entries', 'next_desc']],\n",
    "    how='inner',\n",
    "    on=['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 14)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "across_desc[across_desc['dup_diff'] > 100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C/A</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>SCP</th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>RECOVR AUD</th>\n",
       "      <th>REGULAR</th>\n",
       "      <th>dup_diff</th>\n",
       "      <th>DESC</th>\n",
       "      <th>prev_entries</th>\n",
       "      <th>prev_desc</th>\n",
       "      <th>next_entries</th>\n",
       "      <th>next_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>5870225</td>\n",
       "      <td>2196639</td>\n",
       "      <td>3673586</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5869852.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5870225.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>5870225</td>\n",
       "      <td>2196639</td>\n",
       "      <td>3673586</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>2196639.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>2196693.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>5870426</td>\n",
       "      <td>2196693</td>\n",
       "      <td>3673733</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5870225.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>5870426.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>5870426</td>\n",
       "      <td>2196693</td>\n",
       "      <td>3673733</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>2196693.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>2196699.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>5870458</td>\n",
       "      <td>2196699</td>\n",
       "      <td>3673759</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5870426.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>5870458.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>5870458</td>\n",
       "      <td>2196699</td>\n",
       "      <td>3673759</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>2196699.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>2196731.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>5870469</td>\n",
       "      <td>2196732</td>\n",
       "      <td>3673737</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>2196731.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5870469.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>5870469</td>\n",
       "      <td>2196732</td>\n",
       "      <td>3673737</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>2196732.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5870625.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-01</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>3172052</td>\n",
       "      <td>2224501</td>\n",
       "      <td>947551</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>3171790.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>3172052.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-01</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>3172052</td>\n",
       "      <td>2224501</td>\n",
       "      <td>947551</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>2224501.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>2224575.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-01</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>3172162</td>\n",
       "      <td>2224575</td>\n",
       "      <td>947587</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>3172052.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>3172162.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-01</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>3172162</td>\n",
       "      <td>2224575</td>\n",
       "      <td>947587</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>2224575.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>2224583.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-01</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>3172182</td>\n",
       "      <td>2224583</td>\n",
       "      <td>947599</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>3172162.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>3172182.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-01</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>3172182</td>\n",
       "      <td>2224583</td>\n",
       "      <td>947599</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>2224583.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>2224621.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-01</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>3172188</td>\n",
       "      <td>2224624</td>\n",
       "      <td>947564</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>2224621.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>3172188.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-01</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>3172188</td>\n",
       "      <td>2224624</td>\n",
       "      <td>947564</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>2224624.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>3172288.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-03</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>5100452</td>\n",
       "      <td>1776599</td>\n",
       "      <td>3323853</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5100005.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5100452.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-03</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>5100452</td>\n",
       "      <td>1776599</td>\n",
       "      <td>3323853</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>1776599.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>1776810.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-03</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>5100655</td>\n",
       "      <td>1776810</td>\n",
       "      <td>3323845</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5100452.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>5100655.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-03</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>5100655</td>\n",
       "      <td>1776810</td>\n",
       "      <td>3323845</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>1776810.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>1776824.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-03</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>5100691</td>\n",
       "      <td>1776824</td>\n",
       "      <td>3323867</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5100655.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>5100691.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-03</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>5100691</td>\n",
       "      <td>1776824</td>\n",
       "      <td>3323867</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>1776824.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>1776926.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-03</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>5100705</td>\n",
       "      <td>1776930</td>\n",
       "      <td>3323775</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>1776926.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5100705.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-00-03</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>5100705</td>\n",
       "      <td>1776930</td>\n",
       "      <td>3323775</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>1776930.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>5100804.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-06-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>590174</td>\n",
       "      <td>458770</td>\n",
       "      <td>131404</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>590059.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>590174.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-06-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>590174</td>\n",
       "      <td>458770</td>\n",
       "      <td>131404</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>458770.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>458770.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-06-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>590204</td>\n",
       "      <td>458770</td>\n",
       "      <td>131434</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>590174.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>590204.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-06-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/27/2019</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>590204</td>\n",
       "      <td>458770</td>\n",
       "      <td>131434</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>458770.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>458770.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-06-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>590209</td>\n",
       "      <td>458770</td>\n",
       "      <td>131439</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>590204.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>590209.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-06-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>03:00:00</td>\n",
       "      <td>590209</td>\n",
       "      <td>458770</td>\n",
       "      <td>131439</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>458770.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>458770.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-06-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>590211</td>\n",
       "      <td>458770</td>\n",
       "      <td>131441</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>458770.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>590211.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>N071</td>\n",
       "      <td>R013</td>\n",
       "      <td>00-06-00</td>\n",
       "      <td>34 ST-PENN STA</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>07:00:00</td>\n",
       "      <td>590211</td>\n",
       "      <td>458770</td>\n",
       "      <td>131441</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>458770.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>590255.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>R123</td>\n",
       "      <td>R290</td>\n",
       "      <td>00-00-02</td>\n",
       "      <td>HOUSTON ST</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>16151065</td>\n",
       "      <td>2471</td>\n",
       "      <td>16148594</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>2471.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>16151065.000</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>R123</td>\n",
       "      <td>R290</td>\n",
       "      <td>00-00-02</td>\n",
       "      <td>HOUSTON ST</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>16151065</td>\n",
       "      <td>2471</td>\n",
       "      <td>16148594</td>\n",
       "      <td>RECOVR AUD</td>\n",
       "      <td>2471.000</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>2563.000</td>\n",
       "      <td>REGULAR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      C/A  UNIT       SCP         STATION        DATE      TIME  RECOVR AUD  \\\n",
       "12   N071  R013  00-00-00  34 ST-PENN STA  02/27/2019  19:00:00     5870225   \n",
       "13   N071  R013  00-00-00  34 ST-PENN STA  02/27/2019  19:00:00     5870225   \n",
       "14   N071  R013  00-00-00  34 ST-PENN STA  02/27/2019  23:00:00     5870426   \n",
       "15   N071  R013  00-00-00  34 ST-PENN STA  02/27/2019  23:00:00     5870426   \n",
       "16   N071  R013  00-00-00  34 ST-PENN STA  02/28/2019  03:00:00     5870458   \n",
       "17   N071  R013  00-00-00  34 ST-PENN STA  02/28/2019  03:00:00     5870458   \n",
       "18   N071  R013  00-00-00  34 ST-PENN STA  02/28/2019  07:00:00     5870469   \n",
       "19   N071  R013  00-00-00  34 ST-PENN STA  02/28/2019  07:00:00     5870469   \n",
       "20   N071  R013  00-00-01  34 ST-PENN STA  02/27/2019  19:00:00     3172052   \n",
       "21   N071  R013  00-00-01  34 ST-PENN STA  02/27/2019  19:00:00     3172052   \n",
       "22   N071  R013  00-00-01  34 ST-PENN STA  02/27/2019  23:00:00     3172162   \n",
       "23   N071  R013  00-00-01  34 ST-PENN STA  02/27/2019  23:00:00     3172162   \n",
       "24   N071  R013  00-00-01  34 ST-PENN STA  02/28/2019  03:00:00     3172182   \n",
       "25   N071  R013  00-00-01  34 ST-PENN STA  02/28/2019  03:00:00     3172182   \n",
       "26   N071  R013  00-00-01  34 ST-PENN STA  02/28/2019  07:00:00     3172188   \n",
       "27   N071  R013  00-00-01  34 ST-PENN STA  02/28/2019  07:00:00     3172188   \n",
       "36   N071  R013  00-00-03  34 ST-PENN STA  02/27/2019  19:00:00     5100452   \n",
       "37   N071  R013  00-00-03  34 ST-PENN STA  02/27/2019  19:00:00     5100452   \n",
       "38   N071  R013  00-00-03  34 ST-PENN STA  02/27/2019  23:00:00     5100655   \n",
       "39   N071  R013  00-00-03  34 ST-PENN STA  02/27/2019  23:00:00     5100655   \n",
       "40   N071  R013  00-00-03  34 ST-PENN STA  02/28/2019  03:00:00     5100691   \n",
       "41   N071  R013  00-00-03  34 ST-PENN STA  02/28/2019  03:00:00     5100691   \n",
       "42   N071  R013  00-00-03  34 ST-PENN STA  02/28/2019  07:00:00     5100705   \n",
       "43   N071  R013  00-00-03  34 ST-PENN STA  02/28/2019  07:00:00     5100705   \n",
       "52   N071  R013  00-06-00  34 ST-PENN STA  02/27/2019  19:00:00      590174   \n",
       "53   N071  R013  00-06-00  34 ST-PENN STA  02/27/2019  19:00:00      590174   \n",
       "54   N071  R013  00-06-00  34 ST-PENN STA  02/27/2019  23:00:00      590204   \n",
       "55   N071  R013  00-06-00  34 ST-PENN STA  02/27/2019  23:00:00      590204   \n",
       "56   N071  R013  00-06-00  34 ST-PENN STA  02/28/2019  03:00:00      590209   \n",
       "57   N071  R013  00-06-00  34 ST-PENN STA  02/28/2019  03:00:00      590209   \n",
       "58   N071  R013  00-06-00  34 ST-PENN STA  02/28/2019  07:00:00      590211   \n",
       "59   N071  R013  00-06-00  34 ST-PENN STA  02/28/2019  07:00:00      590211   \n",
       "102  R123  R290  00-00-02      HOUSTON ST  02/28/2019  04:00:00    16151065   \n",
       "103  R123  R290  00-00-02      HOUSTON ST  02/28/2019  04:00:00    16151065   \n",
       "\n",
       "     REGULAR  dup_diff        DESC  prev_entries   prev_desc  next_entries  \\\n",
       "12   2196639   3673586     REGULAR   5869852.000     REGULAR   5870225.000   \n",
       "13   2196639   3673586  RECOVR AUD   2196639.000     REGULAR   2196693.000   \n",
       "14   2196693   3673733     REGULAR   5870225.000  RECOVR AUD   5870426.000   \n",
       "15   2196693   3673733  RECOVR AUD   2196693.000     REGULAR   2196699.000   \n",
       "16   2196699   3673759     REGULAR   5870426.000  RECOVR AUD   5870458.000   \n",
       "17   2196699   3673759  RECOVR AUD   2196699.000     REGULAR   2196731.000   \n",
       "18   2196732   3673737     REGULAR   2196731.000     REGULAR   5870469.000   \n",
       "19   2196732   3673737  RECOVR AUD   2196732.000     REGULAR   5870625.000   \n",
       "20   2224501    947551     REGULAR   3171790.000     REGULAR   3172052.000   \n",
       "21   2224501    947551  RECOVR AUD   2224501.000     REGULAR   2224575.000   \n",
       "22   2224575    947587     REGULAR   3172052.000  RECOVR AUD   3172162.000   \n",
       "23   2224575    947587  RECOVR AUD   2224575.000     REGULAR   2224583.000   \n",
       "24   2224583    947599     REGULAR   3172162.000  RECOVR AUD   3172182.000   \n",
       "25   2224583    947599  RECOVR AUD   2224583.000     REGULAR   2224621.000   \n",
       "26   2224624    947564     REGULAR   2224621.000     REGULAR   3172188.000   \n",
       "27   2224624    947564  RECOVR AUD   2224624.000     REGULAR   3172288.000   \n",
       "36   1776599   3323853     REGULAR   5100005.000     REGULAR   5100452.000   \n",
       "37   1776599   3323853  RECOVR AUD   1776599.000     REGULAR   1776810.000   \n",
       "38   1776810   3323845     REGULAR   5100452.000  RECOVR AUD   5100655.000   \n",
       "39   1776810   3323845  RECOVR AUD   1776810.000     REGULAR   1776824.000   \n",
       "40   1776824   3323867     REGULAR   5100655.000  RECOVR AUD   5100691.000   \n",
       "41   1776824   3323867  RECOVR AUD   1776824.000     REGULAR   1776926.000   \n",
       "42   1776930   3323775     REGULAR   1776926.000     REGULAR   5100705.000   \n",
       "43   1776930   3323775  RECOVR AUD   1776930.000     REGULAR   5100804.000   \n",
       "52    458770    131404     REGULAR    590059.000     REGULAR    590174.000   \n",
       "53    458770    131404  RECOVR AUD    458770.000     REGULAR    458770.000   \n",
       "54    458770    131434     REGULAR    590174.000  RECOVR AUD    590204.000   \n",
       "55    458770    131434  RECOVR AUD    458770.000     REGULAR    458770.000   \n",
       "56    458770    131439     REGULAR    590204.000  RECOVR AUD    590209.000   \n",
       "57    458770    131439  RECOVR AUD    458770.000     REGULAR    458770.000   \n",
       "58    458770    131441     REGULAR    458770.000     REGULAR    590211.000   \n",
       "59    458770    131441  RECOVR AUD    458770.000     REGULAR    590255.000   \n",
       "102     2471  16148594     REGULAR      2471.000     REGULAR  16151065.000   \n",
       "103     2471  16148594  RECOVR AUD      2471.000     REGULAR      2563.000   \n",
       "\n",
       "      next_desc  \n",
       "12   RECOVR AUD  \n",
       "13      REGULAR  \n",
       "14   RECOVR AUD  \n",
       "15      REGULAR  \n",
       "16   RECOVR AUD  \n",
       "17      REGULAR  \n",
       "18   RECOVR AUD  \n",
       "19      REGULAR  \n",
       "20   RECOVR AUD  \n",
       "21      REGULAR  \n",
       "22   RECOVR AUD  \n",
       "23      REGULAR  \n",
       "24   RECOVR AUD  \n",
       "25      REGULAR  \n",
       "26   RECOVR AUD  \n",
       "27      REGULAR  \n",
       "36   RECOVR AUD  \n",
       "37      REGULAR  \n",
       "38   RECOVR AUD  \n",
       "39      REGULAR  \n",
       "40   RECOVR AUD  \n",
       "41      REGULAR  \n",
       "42   RECOVR AUD  \n",
       "43      REGULAR  \n",
       "52   RECOVR AUD  \n",
       "53      REGULAR  \n",
       "54   RECOVR AUD  \n",
       "55      REGULAR  \n",
       "56   RECOVR AUD  \n",
       "57      REGULAR  \n",
       "58   RECOVR AUD  \n",
       "59      REGULAR  \n",
       "102  RECOVR AUD  \n",
       "103     REGULAR  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "across_desc[across_desc['dup_diff'] > 100].head(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible Remedies\n",
    "\n",
    "There are only 34 duplicates with meaningfully different values which we can visually inspect in their entirety. With the exception of HOUSTON ST, the \"RECOVR AUD\" is the correct value based on which value more closely matches the preceeding value and the post-ceeding value.\n",
    "\n",
    "Option 1: Clean this specific data\n",
    "1. Remove Houston ST REGULAR record individually\n",
    "2. Remove all other duplicates, choosing the RECOVR AUD\n",
    "\n",
    "However, this would couple the cleaning notebook to this specific dataset\n",
    "\n",
    "Option 2: Assume RECOVR AUD is always more accurate\n",
    "1. Remove all duplicates, choosing the RECOVR AUD\n",
    "\n",
    "It's unclear if this assumption would hold with other datasets\n",
    "\n",
    "Option 3: Remove any record that has a duplicate (assume it will usually be a small number)\n",
    "1. Remove all records with a duplicate\n",
    "\n",
    "#### Choice: Option 2. Semantically, it seems plausible that a \"recovery\" reading would be the correct one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.3 Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(raw):\n",
    "    raw_sorted = raw.sort_values(['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME', 'DESC'])\n",
    "    return raw_sorted.drop_duplicates(subset=['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dups = remove_duplicates(mta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate\n",
    "no_dups.groupby(['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME'])\\\n",
    "    .agg({ 'ENTRIES': 'count'})\\\n",
    "    .reset_index()\\\n",
    "    .ENTRIES.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ _CHECKPOINT_ ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ: How often should you check point?\n",
    "with open('../src/data/mta_data_march_no_duplicates.pickle', 'wb') as write_file:\n",
    "    pickle.dump(no_dups, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert level_0, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-66e3e26ec84e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../src/data/mta_data_march_no_duplicates.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mno_dups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill)\u001b[0m\n\u001b[1;32m   4707\u001b[0m                 \u001b[0;31m# to ndarray and maybe infer different dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4708\u001b[0m                 \u001b[0mlevel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_casted_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4709\u001b[0;31m                 \u001b[0mnew_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4711\u001b[0m         \u001b[0mnew_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   3589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_duplicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot insert {}, already exists\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert level_0, already exists"
     ]
    }
   ],
   "source": [
    "with open('../src/data/mta_data_march_no_duplicates.pickle', 'rb') as read_file:\n",
    "    no_dups = pickle.load(read_file).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ _CHECKPOINT_ ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Subset Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ: Is this a good thing to do?\n",
    "subset = no_dups[['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'TIME', 'ENTRIES']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Convert Date Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_formats(raw):\n",
    "    raw['datetime'] = pd.to_datetime(raw.DATE + ' ' + raw.TIME)\n",
    "    raw['DATE'] = pd.to_datetime(raw.DATE)\n",
    "    raw['TIME'] = pd.to_datetime(raw.TIME).dt.time\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_formatted_dates = convert_date_formats(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Calculate Entries for Each Time Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interval_data(raw):\n",
    "    sorted_data = raw.sort_values(['C/A', 'UNIT', 'SCP', 'STATION', 'datetime'])\n",
    "    sorted_data['entries_since_last_record'] = sorted_data.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['ENTRIES'].diff(1)\n",
    "    sorted_data['time_interval'] = sorted_data.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['datetime'].diff(1)\n",
    "    return sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_interval_data = add_interval_data(with_formatted_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Handle Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Visualize Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_in_hours = with_interval_data.time_interval.dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(intervals_in_hours, with_interval_data.entries_since_last_record, alpha=0.1)\n",
    "plt.xlabel('Hours in the interval')\n",
    "plt.ylabel('Number of Entries')\n",
    "plt.title('Raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(intervals_in_hours, with_interval_data.entries_since_last_record, alpha=0.1)\n",
    "plt.xlabel('Hours in the interval')\n",
    "plt.ylabel('Number of Entries')\n",
    "plt.title('Zoomed In')\n",
    "plt.xlim([0,150])\n",
    "plt.ylim([-3000, 5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "There does not appear to be a strong relationship between long intervals and high entry values. Therefore, we assume that high entry values and large time intervals are erroneous outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Remove Outliers Within A Turnstile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "Our anlysis is primarily focused on the most trafficed stations. Therefore, we need to be careful removing large outliers. If those presumed outliers are in fact correct, then we are losing our most relevant data.\n",
    "\n",
    "Instead, we will focus on removing outliers _within_ a single turnstile. It is much less likely a single turnstile gets an accurate reading that significantly deviates from the mean. That way we avoid simply removing the most popular turnstiles that are consistently the most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_turnstile_deviations(raw):\n",
    "    raw['unit_mean'] = raw.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['entries_since_last_record'].transform('mean')\n",
    "    raw['unit_std'] = raw.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['entries_since_last_record'].transform('std')\n",
    "    raw['deviations'] = (raw['entries_since_last_record'] - raw['unit_mean']) / raw['unit_std']\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_deviations = add_turnstile_deviations(with_interval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_deviations.describe([0.0001, 0.001, 0.01, 0.1, 0.9, 0.99, 0.999, 0.9999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "Chose a cutoff of 5 standard deviations. We consider that to be a very conservative threshold. 99.9% of the data falls below that value so we don't risk removing the highest readings which are most valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_intra_turnstile_outliers(raw, dev_threshold):\n",
    "    return raw[((raw.deviations > -dev_threshold) & (raw.deviations < dev_threshold)) | raw.deviations.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_turnstile_outliers = remove_intra_turnstile_outliers(with_deviations, dev_threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "perc_removed_so_far = 1 - (no_turnstile_outliers.shape[0] / mta_data.shape[0])\n",
    "print(f\"So far, we've removed {perc_removed_so_far * 100} percent of the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Visualize Remaining Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_interval_in_hours = no_turnstile_outliers.time_interval.dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(new_interval_in_hours, no_turnstile_outliers.entries_since_last_record, alpha=0.2)\n",
    "plt.xlabel('Hours in the interval')\n",
    "plt.ylabel('Number of Entries')\n",
    "plt.title('Raw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(new_interval_in_hours, no_turnstile_outliers.entries_since_last_record, alpha=0.1)\n",
    "plt.xlabel('Hours in the interval')\n",
    "plt.ylabel('Number of Entries')\n",
    "plt.title('Zoomed In')\n",
    "plt.xlim([0,150])\n",
    "plt.ylim([-3000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_turnstile_outliers.describe([0.0001, 0.001, 0.005, 0.01, 0.1, 0.9, 0.99, 0.999, 0.9999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "For both `entries_since_last_record` and `time_interval`, there is a massive gap between the 99.99% percentile and the maximum. That is also true of the gap between the 0.01% percentile and the minimum for `entries_since_last_record`.\n",
    "\n",
    "We want to choose a threshold between those values.\n",
    "\n",
    "Also, any interval less than a minute is almost certainly not relevant. Either the reading is accurate and it adds a negligible number of entrants or it's an error. Therefore, we should remove those low intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Remove Extreme Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extreme_entries(mta):   \n",
    "     return mta[\n",
    "         (mta.entries_since_last_record < mta.entries_since_last_record.quantile(0.99995)) &\n",
    "         (mta.entries_since_last_record > mta.entries_since_last_record.quantile(1 - 0.99995))\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extreme_intervals(mta):   \n",
    "     return mta[\n",
    "         (mta.time_interval < mta.time_interval.quantile(0.99995)) & \n",
    "         (mta.time_interval > mta.time_interval.quantile(0.01))\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_extremes = remove_extreme_intervals(remove_extreme_entries(no_turnstile_outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_in_hours = no_extremes.time_interval.dt.total_seconds() / 3600\n",
    "\n",
    "plt.scatter(interval_in_hours, no_extremes.entries_since_last_record, alpha=0.1)\n",
    "plt.xlabel('Hours in the interval')\n",
    "plt.ylabel('Number of Entries')\n",
    "plt.title('With No Extreme Outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_removed = 1 - (no_extremes.shape[0] / mta_data.shape[0])\n",
    "print(f'So far we have removed {np.round(perc_removed * 100, 2)} percent of the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_extremes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_extremes = no_extremes.drop(columns=['deviations', 'unit_mean', 'unit_std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ _CHECKPOINT_ ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ: How often should you check point?\n",
    "with open('../src/data/mta_data_march_no_extremes.pickle', 'wb') as write_file:\n",
    "    pickle.dump(no_extremes, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/data/mta_data_march_no_extremes.pickle', 'rb') as read_file:\n",
    "    no_extremes = pickle.load(read_file).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ _CHECKPOINT_ ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Investigate Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 How Many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_extremes[no_extremes.entries_since_last_record < 0].shape[0] / no_extremes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_extremes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "In a real world setting, we would simply drop the <1% of data points that are negative. But in order to practice, we kept investigating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Consistently Negative Turnstiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "A number of the turnstiles appear to _consistently_ have negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_extremes['always_decreasing'] = no_extremes.groupby(['C/A', 'UNIT', 'SCP', 'STATION'])['ENTRIES'].transform(lambda s: s.is_monotonic_decreasing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "always_decreasing = no_extremes[\n",
    "    (no_extremes.entries_since_last_record < 0) & # has negative entries for a time period\n",
    "    no_extremes.always_decreasing # the turnstile the record is from *always* has negative entries\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "always_decreasing.describe([0.0001, 0.001, 0.01, 0.1, 0.9, 0.99, 0.999, 0.9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_in_hours = always_decreasing.time_interval.dt.total_seconds() / 3600\n",
    "\n",
    "plt.scatter(interval_in_hours, always_decreasing.entries_since_last_record, alpha=0.1)\n",
    "plt.xlabel('Hours in the interval')\n",
    "plt.ylabel('Number of Entries')\n",
    "plt.title('Raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_of_neg_on_always_decr_turnstiles = always_decreasing.shape[0] / no_extremes[no_extremes.entries_since_last_record < 0].shape[0]\n",
    "print(f'{perc_of_neg_on_always_decr_turnstiles * 100} of the negative values in the data are on turnstiles that alway have negative readings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The vast majority of the negative values in the data set are on turnstiles that consistently and uniformly have their counters going down. In addition, their absolute values are reasonable and fall within the range of positive values observed in the data. Finally, the vast majority of the records have the expected 4 hour interval.\n",
    "\n",
    "Therefore, we assume these readings are accurate with simply a flipped sign. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 Flip Reasonable Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_reasonable_negative_entries_since_last_interval(mta):\n",
    "    \"\"\"Finds all \"reasonable\" negative values for `entries_since_last_interval` and corrects them to be positive.\n",
    "\n",
    "    A record is considered to be a \"reasonable\" negative if it meets the following criteria:\n",
    "        - it is on a turnstile which exclusively has decreasing entrance readings\n",
    "        - it is NOT in the lowest 1% of negative readings\n",
    "        - it has an interval of 4hours\n",
    "\n",
    "    Args:\n",
    "        param1 (dataframe): the MTA data\n",
    "\n",
    "    Returns:\n",
    "        dataframe: the corrected dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # QQ: Seems very side effect-y. Is it worth it to copy the df first?\n",
    "    mta['entries_since_last_record'] = np.where(\n",
    "        (\n",
    "            (mta.entries_since_last_record < 0) &\n",
    "            mta.always_decreasing &\n",
    "            (mta.entries_since_last_record > mta[mta.entries_since_last_record < 0].entries_since_last_record.quantile(0.01)) &\n",
    "            (mta.time_interval == pd.Timedelta('4 hours'))\n",
    "        ),\n",
    "        np.abs(no_extremes.entries_since_last_record),\n",
    "        no_extremes.entries_since_last_record\n",
    "    )\n",
    "    return mta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_negatives = flip_reasonable_negative_entries_since_last_interval(no_extremes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.4 Drop Remaining Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_negatives(mta):\n",
    "    return mta[mta.entries_since_last_record >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_negatives = drop_negatives(flipped_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.5 Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'So far we have removed {np.round(100-(no_negatives.shape[0] / mta_data.shape[0] * 100), 2)} percent of the data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(no_negatives.time_interval.dt.total_seconds() / 3600, no_negatives.entries_since_last_record, alpha=0.1)\n",
    "plt.xlabel('Hours in the interval')\n",
    "plt.ylabel('Number of Entries')\n",
    "plt.title('Without Negatives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_negatives = no_negatives.drop(columns=['always_decreasing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ _CHECKPOINT_ ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ: How often should you check point?\n",
    "with open('../src/data/mta_data_march_no_negatives.pickle', 'wb') as write_file:\n",
    "    pickle.dump(no_negatives, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/data/mta_data_march_no_negatives.pickle', 'rb') as read_file:\n",
    "    no_negatives = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ _CHECKPOINT_ ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Intervals That Cross Days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 How many readings overlap a day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_than_one_day = no_negatives[no_negatives.datetime.dt.hour < (no_negatives.time_interval.dt.total_seconds() / 3600)]\n",
    "\n",
    "f'{np.round(more_than_one_day.shape[0] / no_negatives.shape[0] * 100, 2)} percent of records overlap days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "and_at_midnight = more_than_one_day[more_than_one_day.TIME == datetime.time(0, 0)]\n",
    "f'Of those, {np.round(and_at_midnight.shape[0] / more_than_one_day.shape[0] * 100, 2)} percent are readings at midnight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_over_24_hours = more_than_one_day[more_than_one_day.time_interval.dt.total_seconds() >= (24 * 3600)].shape[0]\n",
    "\n",
    "f'And {np.round(num_over_24_hours / more_than_one_day.shape[0] * 100, 2)} percent are over 24 hours'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "The majority of the records are readings at midnight. We can simply adjust their time reading to 23:59 the previous day to ensure the record falls on the date in which people entered. The information of the DATE and TIME fields are redundant and we can drop those fields in favor of the `datetime` column.\n",
    "\n",
    "Less than 2% of the records that span more than one day cover more than 24 hours. Considering that represents less than 0.001% of the entire data set, we will simply remove those records.\n",
    "\n",
    "With the rest, we will proportionally distribute the entrants based on how many hours the interval was in each day. This will allow us to aggregate more accurate daily readings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 Prepare Date Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_intervals_gt_day(mta):\n",
    "    return mta[mta.time_interval < pd.Timedelta('24 hours')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_columns(mta):\n",
    "    mta['start_datetime'] = mta.datetime - mta.time_interval\n",
    "    return mta.rename(columns={ 'datetime': 'end_datetime'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_gt_24 = remove_intervals_gt_day(no_negatives)\n",
    "no_gt_24 = add_start_end_columns(no_gt_24).drop(columns=['DATE', 'TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_midnight_to_day_before(mta):\n",
    "    mta['end_datetime'] = np.where(\n",
    "        mta.end_datetime.dt.hour == 0,\n",
    "        mta.end_datetime - pd.Timedelta('1 second'),\n",
    "        mta.end_datetime\n",
    "    )\n",
    "    return mta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_midnight = move_midnight_to_day_before(no_gt_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3 Split Entries That Overlap Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_interval_split = no_midnight.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_interval_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_datetime_to_midnight(date):\n",
    "    return date.replace(hour=0, minute=0, second=0)\n",
    "\n",
    "def create_next_day_records(mta):\n",
    "    new_records = mta[mta.start_datetime.dt.day != mta.end_datetime.dt.day].copy()\n",
    "    \n",
    "    new_records['start_datetime'] = new_records.end_datetime.map(set_datetime_to_midnight)\n",
    "    new_records['interval_on_that_day'] = new_records.end_datetime - new_records.start_datetime\n",
    "    \n",
    "    new_records['entries_since_last_record'] = new_records.entries_since_last_record * (new_records.interval_on_that_day / new_records.time_interval)\n",
    "    new_records['new_time_interval'] = new_records['interval_on_that_day']\n",
    "\n",
    "    return new_records.drop(columns=['interval_on_that_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_datetime_to_end_of_day(date):\n",
    "    return (date - pd.Timedelta('1 day')).replace(hour=23, minute=59, second=59)\n",
    "\n",
    "def adjust_current_day(mta):\n",
    "    mta['end_datetime'] = np.where(\n",
    "        mta.start_datetime.dt.day != mta.end_datetime.dt.day,\n",
    "        mta.end_datetime.map(set_datetime_to_end_of_day),\n",
    "        mta.end_datetime\n",
    "    )\n",
    "    mta['interval_on_that_day'] = mta.end_datetime - mta.start_datetime\n",
    "    \n",
    "    mta['entries_since_last_record'] = np.where(\n",
    "        mta.start_datetime.dt.day != mta.end_datetime.dt.day,\n",
    "        mta.entries_since_last_record * (mta.interval_on_that_day / mta.time_interval),\n",
    "        mta.entries_since_last_record\n",
    "    )\n",
    "    mta['new_time_interval'] = mta['interval_on_that_day']\n",
    "    \n",
    "    return mta.drop(columns=['interval_on_that_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_overlapping_intervals(mta):\n",
    "    records_to_add = create_next_day_records(mta)\n",
    "    with_new = mta.append(records_to_add)\n",
    "    return adjust_current_day(with_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_split = split_overlapping_intervals(no_midnight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.4 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_before_split = pre_interval_split.shape[0]\n",
    "num_across_days = pre_interval_split[pre_interval_split.start_datetime.dt.day != pre_interval_split.end_datetime.dt.day].shape[0]\n",
    "num_in_result = with_split.shape[0]\n",
    "\n",
    "assert num_in_result == (num_before_split + num_across_days), 'Unexpected number of records'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_split[with_split.start_datetime.dt.day != with_split.end_datetime.dt.day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = with_split.entries_since_last_record.sum() / pre_interval_split.entries_since_last_record.sum()\n",
    "f'Splitting and proportionally assigning entrants added {np.round((ratio -1) * 100, 2)} percent to the total entries'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Store Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = with_split[['C/A', 'UNIT', 'SCP', 'STATION', 'start_datetime', 'end_datetime', 'time_interval', 'entries_since_last_record']]\\\n",
    "                     .rename(columns={ 'entries_since_last_record': 'entries' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/data/mta_data_march_cleaned.pickle', 'wb') as write_file:\n",
    "    pickle.dump(cleaned, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.to_csv('../src/data/mta_data_march_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
