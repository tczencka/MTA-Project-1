{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "# various options in pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/data/mta_data_march_cleaned.pickle', 'rb') as read_file:\n",
    "    clean_columns=pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze weekday vs. weekend entries per day by station\n",
    "\n",
    "This is a proxy method to deprioritize stations with higher share of tourists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C/A</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>SCP</th>\n",
       "      <th>STATION</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>time_interval</th>\n",
       "      <th>entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>2019-02-23 03:00:00</td>\n",
       "      <td>2019-02-23 07:00:00</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>11.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>2019-02-23 07:00:00</td>\n",
       "      <td>2019-02-23 11:00:00</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>60.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>2019-02-23 11:00:00</td>\n",
       "      <td>2019-02-23 15:00:00</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>160.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>2019-02-23 15:00:00</td>\n",
       "      <td>2019-02-23 19:00:00</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>290.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>2019-02-23 19:00:00</td>\n",
       "      <td>2019-02-23 23:00:00</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>143.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C/A  UNIT       SCP STATION      start_datetime        end_datetime  \\\n",
       "0  A002  R051  02-00-00   59 ST 2019-02-23 03:00:00 2019-02-23 07:00:00   \n",
       "1  A002  R051  02-00-00   59 ST 2019-02-23 07:00:00 2019-02-23 11:00:00   \n",
       "2  A002  R051  02-00-00   59 ST 2019-02-23 11:00:00 2019-02-23 15:00:00   \n",
       "3  A002  R051  02-00-00   59 ST 2019-02-23 15:00:00 2019-02-23 19:00:00   \n",
       "4  A002  R051  02-00-00   59 ST 2019-02-23 19:00:00 2019-02-23 23:00:00   \n",
       "\n",
       "  time_interval  entries  \n",
       "0      04:00:00   11.000  \n",
       "1      04:00:00   60.000  \n",
       "2      04:00:00  160.000  \n",
       "3      04:00:00  290.000  \n",
       "4      04:00:00  143.000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe that is copy of cleaned data to work with\n",
    "clean_columns_tim_analysis = clean_columns.copy()\n",
    "clean_columns_tim_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_columns_tim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-faa08762ebe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# adding column with only date so we can group rows by date for analysis of average daily entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m clean_columns_tim_analysis['date_edit'] = (pd.to_datetime(clean_columns_tim['start_datetime'].\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                           apply(lambda x: x.strftime('%m/%d/%y'))))\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# creating function to categorize date as weekday vs. weekend categorization so we can group rows and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_columns_tim' is not defined"
     ]
    }
   ],
   "source": [
    "# adding column with only date so we can group rows by date for analysis of average daily entries\n",
    "clean_columns_tim_analysis['date_edit'] = (pd.to_datetime(clean_columns_tim['start_datetime'].\n",
    "                                                          apply(lambda x: x.strftime('%m/%d/%y'))))\n",
    "\n",
    "# creating function to categorize date as weekday vs. weekend categorization so we can group rows and \n",
    "# calc avg daily entries in weekday vs. weekend\n",
    "def weekday_vs_weekend(dayInt):\n",
    "    if dayInt in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']:\n",
    "        return 'Weekday'\n",
    "    elif dayInt in ['Saturday', 'Sunday']:\n",
    "        return 'Weekend'\n",
    "\n",
    "# add column in dataframe to identify whether row's entries belong to weekday vs. weekend; so that we can \n",
    "# sum those entries to weekend vs. weekend for calculating avg entries per day type for that station\n",
    "# note to self: dt.day_name is datetime method to convert raw datetime data into a weekday name; then use 'apply lambda' to iterate row by row and run previously defined function\n",
    "clean_columns_tim_analysis['day_of_week'] = clean_columns_tim_analysis['start_datetime'].dt.day_name().apply(lambda x: weekday_vs_weekend(x))\n",
    "\n",
    "# ask John why doesnt below work... where does .dt go vs. .datetime; whats the natural language behind these\n",
    "#clean_columns_tim['day_of_week'] = weekday_vs_weekend(clean_columns_tim['start_datetime'].dt.day_name()) 2* df['column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'day_of_week'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cfb7aff46a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# divide by count of rows to get average daily entries because denominator is # of internals, not # of days\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# clean_columns_tim_analysisv2 dataframe groups by date_edit so that each row is a day; entries is sum of entries by intervals in that day\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclean_columns_tim_analysisv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_columns_tim_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STATION'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'day_of_week'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_edit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'entries'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m   7892\u001b[0m             \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7893\u001b[0m             \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7894\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7895\u001b[0m         )\n\u001b[1;32m   7896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   2520\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid type: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mmutated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m             )\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'day_of_week'"
     ]
    }
   ],
   "source": [
    "# create new dataframe 'v2' after finding error - previous data frame each row is a time interval; cannot \n",
    "# divide by count of rows to get average daily entries because denominator is # of internals, not # of days\n",
    "# clean_columns_tim_analysisv2 dataframe groups by date_edit so that each row is a day; entries is sum of entries by intervals in that day\n",
    "clean_columns_tim_analysisv2 = clean_columns_tim_analysis.groupby(['STATION', 'day_of_week', 'date_edit'], as_index=False).agg({'entries': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_columns_timv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe to group station data by day of week - this will allow us to divide entries by count of days\n",
    "# in order to calculate average entries per day\n",
    "weekdays_analysis = clean_columns_timv2.groupby(['STATION', 'day_of_week'], as_index=False).agg({'date_edit': 'count', 'entries': 'sum'}).copy()\n",
    "weekdays_analysis = weekdays_analysis.rename(columns={\"date_edit\": \"count_of_days_in_data\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column with average entries per day (each station has separate data for weekend vs. weekday)\n",
    "weekdays_analysis['entries_per_day'] = weekdays_analysis['entries'] / weekdays_analysis['count_of_days_in_data'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since stackbar chart in matplotlib requires graphing top vs. bottom stack as top separate series, this\n",
    "# code is to separate dataframe into two series \n",
    "\n",
    "# Get names of stations with highest entries from other analysis (Tyler)\n",
    "highest_entry_stations = ['34 ST-PENN STA',\n",
    " 'GRD CNTRL-42 ST',\n",
    " '34 ST-HERALD SQ',\n",
    " '23 ST',\n",
    " '42 ST-PORT AUTH',\n",
    " '14 ST-UNION SQ',\n",
    " 'TIMES SQ-42 ST',\n",
    " 'FULTON ST',\n",
    " '86 ST',\n",
    " '125 ST',\n",
    " 'CANAL ST',\n",
    " '59 ST COLUMBUS',\n",
    " '59 ST',\n",
    " '14 ST',\n",
    " '96 ST',\n",
    " 'CHAMBERS ST',\n",
    " 'FLUSHING-MAIN',\n",
    " '47-50 STS ROCK',\n",
    " 'JKSN HT-ROOSVLT',\n",
    " '50 ST']\n",
    "\n",
    "# use filter to identify the set of stations we want to pull\n",
    "#df.loc[df.gender == 'Male', ]\n",
    "stations_to_graph = weekdays_analysis.loc[(weekdays_analysis['STATION'].isin(list(highest_entry_stations)))]\n",
    "\n",
    "# separate series for weekday vs. weekend - while making sure to sort by station so both output series are tied\n",
    "# example filtering code from class: df[df.age > 50].head(4)\n",
    "stations_to_graph_weekday = stations_to_graph[stations_to_graph.day_of_week == 'Weekday'].sort_values(['STATION'])['entries_per_day']\n",
    "stations_to_graph_weekend = stations_to_graph[stations_to_graph.day_of_week == 'Weekend'].sort_values(['STATION'])['entries_per_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we are graphing 20 stations (should have 40 rows, two datapoints per station)\n",
    "stations_to_graph.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it pulled in the right number of stations\n",
    "len(stations_to_graph['STATION'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GRAPH #1 - Stack bar of weekday vs. weekend <- decided not to use because visual is not compelling\n",
    "\n",
    "Sample code below for how to graph stack bar\n",
    "N = 5\n",
    "menMeans = (20, 35, 30, 35, 27)\n",
    "womenMeans = (25, 32, 34, 20, 25)\n",
    "menStd = (2, 3, 4, 1, 2)\n",
    "womenStd = (3, 5, 2, 3, 3)\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, menMeans, width, yerr=menStd)\n",
    "p2 = plt.bar(ind, womenMeans, width,\n",
    "             bottom=menMeans, yerr=womenStd)\"\"\"\n",
    "\n",
    "N = len(stations_to_graph_weekday)\n",
    "ind = np.arange(N)\n",
    "width = 0.35\n",
    "\n",
    "p1 = plt.bar(ind, stations_to_graph_weekday)\n",
    "p2 = plt.bar(ind, stations_to_graph_weekend,bottom=stations_to_graph_weekday.values)\n",
    "print(sorted(highest_entry_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GRAPH #2 - 100% stack bar of weekday vs. weekend <- decided not to use because weekday + weekend entries per \n",
    "day as 100% stack has no real world meaning (confusing)\n",
    "\n",
    "Sample code below for how to create 100% stack bar\n",
    "# Data\n",
    "r = [0,1,2,3,4]\n",
    "raw_data = {'greenBars': [20, 1.5, 7, 10, 5], 'orangeBars': [5, 15, 5, 10, 15],'blueBars': [2, 15, 18, 5, 10]}\n",
    "df = pd.DataFrame(raw_data)\n",
    " \n",
    "# From raw value to percentage\n",
    "totals = [i+j+k for i,j,k in zip(df['greenBars'], df['orangeBars'], df['blueBars'])]\n",
    "greenBars = [i / j * 100 for i,j in zip(df['greenBars'], totals)]\n",
    "orangeBars = [i / j * 100 for i,j in zip(df['orangeBars'], totals)]\n",
    "blueBars = [i / j * 100 for i,j in zip(df['blueBars'], totals)]\n",
    " \n",
    "# plot\n",
    "barWidth = 0.85\n",
    "names = ('A','B','C','D','E')\n",
    "# Create green Bars\n",
    "plt.bar(r, greenBars, color='#b5ffb9', edgecolor='white', width=barWidth)\n",
    "# Create orange Bars\n",
    "plt.bar(r, orangeBars, bottom=greenBars, color='#f9bc86', edgecolor='white', width=barWidth)\n",
    "# Create blue Bars\n",
    "plt.bar(r, blueBars, bottom=[i+j for i,j in zip(greenBars, orangeBars)], color='#a3acff', edgecolor='white', width=barWidth)\n",
    " \n",
    "# Custom x axis\n",
    "plt.xticks(r, names)\n",
    "plt.xlabel(\"group\")\n",
    " \n",
    "# Show graphic\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# from raw entries per day values to percentage\n",
    "totals = [i+j for i,j in zip(stations_to_graph_weekday, stations_to_graph_weekend)]\n",
    "blueBars = [(i / j) * 100 for i,j in zip(stations_to_graph_weekday, totals)]\n",
    "orangeBars = [(i / j) * 100 for i,j in zip(stations_to_graph_weekend, totals)]\n",
    "\n",
    "# Custom chart settings\n",
    "plt.figure(figsize = [30,15]) # [width, height]\n",
    "plt.title('Station weekday vs. weekend mix')\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Custom x axis settings\n",
    "N = len(stations_to_graph_weekday)\n",
    "ind = np.arange(N)\n",
    "width = 0.35\n",
    "names = sorted(highest_entry_stations)\n",
    "plt.xticks(ind, names, rotation=90)\n",
    "plt.xlabel('Stations')\n",
    "\n",
    "# Custom y axis settings\n",
    "plt.ylabel('Percentage of entries')\n",
    "\n",
    "# Plot data\n",
    "plt.bar(ind, blueBars, color='tab:blue', label = 'Weekday')\n",
    "plt.bar(ind, orangeBars,bottom=blueBars, color = 'lightgray', label = 'Weekend')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graph #3 - THIS IS THE CORRECT VERSION. Show weekend as multiple of weekday\n",
    "\"\"\"\n",
    "\n",
    "# Only need one bar - how many more times is weekday entries per day greater than weekend entries per day\n",
    "# Note to self: Use zip function to pull data from two separate lists (weekday entries and weekend entries by station) and list comprehension to do the calc\n",
    "blueBars = [(i / j) for i,j in zip(stations_to_graph_weekday, stations_to_graph_weekend)]\n",
    "\n",
    "# Custom chart settings\n",
    "plt.figure(figsize = [30,15]) # [width, height]\n",
    "plt.title('Weekday daily entries as multiple of weekend daily entries')\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Custom x axis settings\n",
    "N = len(stations_to_graph_weekday)\n",
    "ind = np.arange(N)\n",
    "width = 0.35\n",
    "names = sorted(stations_from_tyler)\n",
    "plt.xticks(ind, names, rotation=90)\n",
    "plt.xlabel('Station name')\n",
    "\n",
    "# Plot data\n",
    "p1 = plt.bar(ind, blueBars, color='tab:blue', label = 'Weekday')\n",
    "plt.show\n",
    "\n",
    "# Adjust output image so x-axis labels are not cut off\n",
    "plt.gcf().subplots_adjust(bottom=0.4)\n",
    "\n",
    "# Save plot as png\n",
    "plt.savefig('weekdays_analysis.png', format='png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check vs. high level numbers\n",
    "\n",
    "Check that total stations counts, total entries counts, top 20 as % of total, and top 20 as multiple of other stations pass sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_analysis_temp = weekdays_analysis.loc[weekdays_analysis['day_of_week'] == 'Weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_analysis_temp = weekdays_analysis_temp.sort_values('entries_per_day', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_analysis_temp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weekdays_analysis_temp['entries_per_day'].iloc[0:20].mean())\n",
    "print(weekdays_analysis_temp['entries_per_day'].iloc[20:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_analysis_temp['entries'].iloc[0:20].sum() / weekdays_analysis_temp['entries'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays_analysis_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weekdays_analysis.loc[weekdays_analysis['day_of_week'] == 'Weekday'].\n",
    " sort_values('entries_per_day', ascending=False).\n",
    " iloc[20:]['entries_per_day'].mean()) / (weekdays_analysis.loc[weekdays_analysis['day_of_week'] == 'Weekend'].\n",
    " sort_values('entries_per_day', ascending=False).\n",
    " iloc[20:]['entries_per_day'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weekdays_analysis.loc[weekdays_analysis['day_of_week'] == 'Weekday'].\n",
    " sort_values('entries_per_day', ascending=False).\n",
    " iloc[20:]['entries_per_day'].sum()) / (weekdays_analysis.loc[weekdays_analysis['day_of_week'] == 'Weekend'].\n",
    " sort_values('entries_per_day', ascending=False).\n",
    " iloc[20:]['entries_per_day'].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
